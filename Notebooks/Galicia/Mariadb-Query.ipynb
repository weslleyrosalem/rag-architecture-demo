{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f277ec-ce41-44b0-b66c-f4191af61590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in /opt/app-root/lib/python3.9/site-packages (8.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q einops==0.7.0 langchain==0.1.9 pypdf==4.0.2 sentence-transformers==2.4.0\n",
    "!pip install mysql-connector-python\n",
    "\n",
    "import os\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5df462-20b6-4774-85c0-9331cb90a019",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 2.4.0.dev0, however, your version is 2.4.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API key\n",
    "OPENAI_API_KEY = \"\"  # Replace with your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# Model parameters\n",
    "MAX_TOKENS = 1024\n",
    "TOP_P = 0.95\n",
    "TEMPERATURE = 0.01\n",
    "PRESENCE_PENALTY = 1.03\n",
    "\n",
    "# Embedding model configuration\n",
    "model_kwargs = {'trust_remote_code': True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc6c6eae-5534-4c88-b83d-cde53b5c2c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Connect to MariaDB and load documents\n",
    "db_connection = mysql.connector.connect(\n",
    "    host=\"mariadb.banco-galicia.svc.cluster.local\",\n",
    "    user=\"\",\n",
    "    password=\"\",\n",
    "    database=\"\"\n",
    ")\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Fetch documents from the database\n",
    "cursor.execute(\"SELECT source, content FROM pdf_documents\")\n",
    "rows = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd8981e-2c5c-4b5c-8ae0-84ea00b0ea7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m200.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/app-root/lib/python3.9/site-packages (from faiss-cpu) (23.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from faiss-cpu) (1.26.4)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0.post1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu\n",
    "\n",
    "# Create Document objects\n",
    "documents = []\n",
    "for row in rows:\n",
    "    source, content = row\n",
    "    metadata = {'source': source}\n",
    "    doc = Document(page_content=content, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "\n",
    "# Create a vector store using FAISS\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3011f3d-2c1d-4f34-aad5-75f2907a1bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the system prompt\n",
    "system_template = \"\"\"You are a helpful, respectful, and honest assistant named HatBot answering questions.\n",
    "You will be given a question and a context to provide you with information. You must answer the question based as much as possible on this context.\n",
    "Always answer as helpfully as possible while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense or is not factually coherent, explain why instead of providing incorrect information. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "\n",
    "# Define the user prompt\n",
    "human_template = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "775f8732-cada-4d83-bf4b-ca2751a73802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n",
      "WARNING! presence_penalty is not default parameter.\n",
      "                    presence_penalty was transferred to model_kwargs.\n",
      "                    Please confirm that presence_penalty is what you intended.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the OpenAI ChatGPT model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name=\"gpt-4o\",\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 4}\n",
    "    ),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86d21476-a58d-4311-bbe9-e81ce8b374a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El ARTÍCULO SEGUNDO del estatuto de Loma Negra Compañía Industrial Argentina Sociedad Anónima establece lo siguiente:\n",
      "\n",
      "1. **Domicilio Legal**: La sociedad tiene su domicilio legal en la Ciudad Autónoma de Buenos Aires.\n",
      "2. **Filiales y Sucursales**: A pesar de tener su domicilio principal en Buenos Aires, el Directorio de la sociedad tiene la facultad de establecer filiales, agencias, sucursales y domicilios especiales en cualquier punto de la República Argentina o en el extranjero.\n",
      "3. **Capital Determinado**: El Directorio puede decidir si estas filiales, agencias o sucursales tendrán un capital determinado o no.\n",
      "4. **Leyes Extranjeras**: El domicilio fijado en Buenos Aires se establece sin perjuicio de las leyes de otros países que puedan afectar a las filiales, sucursales y agencias establecidas fuera de Argentina.\n",
      "\n",
      "En resumen, este artículo define el domicilio principal de la sociedad y otorga al Directorio la flexibilidad para expandir sus operaciones tanto dentro como fuera del país, respetando las legislaciones locales correspondientes."
     ]
    }
   ],
   "source": [
    "# Question\n",
    "question = \"Explícame sobre el ARTÍCULO SEGUNDO del estatuto Loma Negra?\"\n",
    "result = qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72507c8a-4585-410c-a565-786092f32595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove duplicate documents\n",
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    sources = set()\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in sources:\n",
    "            unique_list.append(item)\n",
    "            sources.add(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "# Process and print results\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
